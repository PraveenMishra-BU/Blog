Why ReLU (Rectified Linear Unit) activation funtion is so popular in Deep Neural Network? 
